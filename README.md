# ğŸ Snake AI Bot

An advanced Snake game implementation featuring multiple AI approaches, including a perfect Hamilton cycle algorithm and a Deep Q-Learning neural network.

## ğŸŒŸ Features

- **ğŸ® Manual Play Mode** - Control the snake yourself with smooth animations
- **ğŸ¤– Hamilton Cycle Algorithm** - Watch a perfect AI that never dies and completes every grid
- **ğŸ§  Deep Q-Learning AI** - Train a neural network to play Snake using reinforcement learning
- **ğŸ“Š Advanced State Representation** - 124-dimensional state space with spatial awareness
- **ğŸ¯ Dueling DQN Architecture** - Separate value and advantage streams for better learning
- **âš¡ Prioritized Experience Replay** - Learn from important experiences more efficiently
- **ğŸ“ˆ Real-time Visualization** - Watch training progress with smooth pygame rendering
- **ğŸ”„ Resizable Windows** - Drag to resize or press F11 for fullscreen
- **ğŸ—ºï¸ Hamilton Path Overlay** - Toggle with 'H' to see the optimal path visualization

## ğŸ“‹ Table of Contents

- [Installation](#installation)
- [Quick Start](#quick-start)
- [Game Modes](#game-modes)
- [Architecture](#architecture)
- [Training Details](#training-details)
- [Project Structure](#project-structure)
- [Controls](#controls)
- [Configuration](#configuration)
- [Technical Details](#technical-details)
- [Performance](#performance)

## ğŸš€ Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager

### Install Dependencies

```bash
pip install -r requirements.txt
```

Required packages:
- `torch>=2.0.0` - Deep learning framework
- `numpy>=1.24.0` - Numerical computations
- `pygame>=2.5.0` - Game rendering and visualization
- `matplotlib>=3.7.0` - Plotting training metrics

## ğŸ¯ Quick Start

Run the main program:

```bash
python main.py
```

You'll see an interactive menu with 5 options:

```
ğŸ  SNAKE AI GAME  ğŸ

1. ğŸ® Play Manually (You control the snake)
2. ğŸ¤– Watch Hamilton Cycle Demo (Perfect AI play)
3. ğŸ§  Train Neural Network (Deep Q-Learning)
4. ğŸ‘€ Watch Trained Model Play
5. ğŸšª Exit
```

## ğŸ® Game Modes

### 1. Manual Play ğŸ®

Control the snake yourself and try to beat your high score!

**Controls:**
- `Arrow Keys` or `WASD` - Move the snake
- `R` or `Space` - Restart game
- `H` - Toggle Hamilton path overlay
- `F11` - Toggle fullscreen
- `ESC` - Exit

**Features:**
- Smooth interpolated movement
- Resizable window
- Real-time scoring
- Hamilton path visualization (optional)

### 2. Hamilton Cycle Demo ğŸ¤–

Watch an AI that follows a predetermined Hamiltonian cycle - a path that visits every cell exactly once.

**How it works:**
1. Uses Prim's algorithm to generate a minimum spanning tree on a half-resolution grid
2. Navigates around the tree edges to create a Hamiltonian cycle
3. Snake follows this path indefinitely, eventually filling the entire grid
4. Guaranteed to never die (unless interrupted)

**Features:**
- Works on ANY grid size (even, odd, rectangular)
- Generates unique random patterns each time
- Optional path visualization overlay
- Configurable speed

**Use Cases:**
- Baseline comparison for AI training
- Demonstrate solvability of Snake
- Generate training data for neural networks

### 3. Train Neural Network ğŸ§ 

Train a Deep Q-Learning agent from scratch using reinforcement learning.

**Training Features:**
- **Dueling Double DQN Architecture**: Separates state value from action advantages
- **Prioritized Experience Replay**: Learns from important experiences more frequently
- **Adaptive Epsilon Decay**: Adjusts exploration based on performance
- **Multi-step Danger Detection**: Looks ahead 1 and 2 cells
- **Spatial Awareness**: Full grid representation with CNN processing
- **Hamilton Guidance**: Optional teacher policy for faster learning
- **Advanced Reward Shaping**: Encourages safe, strategic play

**State Representation (124 features):**
- Immediate danger (3): collision risk 1 cell ahead
- Lookahead danger (3): collision risk 2 cells ahead
- Current direction (4): one-hot encoded
- Food direction (4): relative position to head
- Full grid (100): spatial awareness (10Ã—10 grid)
  - Head = 1.5
  - Body = 1.0
  - Tail = 0.5
  - Food = 2.0
- Snake length (1): normalized
- Accessible space (1): reachable empty cells
- Can reach tail (1): connectivity check
- Path to food (1): BFS distance
- Space after eating (1): lookahead space
- Hamilton direction (4): optimal path guidance
- Should follow Hamilton (1): binary flag

**Training Options:**
- Headless mode (fast) or visual mode (watchable)
- Configurable grid size
- Adjustable training episodes
- Auto-saves best models

### 4. Watch Trained Model ğŸ‘€

Load a saved model and watch it play multiple games.

**Features:**
- Visualize learned strategies
- Compare performance across different models
- Optional Hamilton path overlay
- Configurable playback speed
- Statistics tracking

## ğŸ—ï¸ Architecture

### Deep Q-Network (DQN) Structure

```
Input (124 features)
    â†“
Split into Grid (100) and Non-Grid (24)
    â†“                         â†“
CNN Branch               Feature Branch
    â†“                         â†“
Conv2d(1â†’16, 3Ã—3)           Pass through
    â†“
MaxPool(2Ã—2)
    â†“
Conv2d(16â†’32, 3Ã—3)
    â†“
AdaptiveAvgPool
    â†“
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
        Concatenate (56)
              â†“
         FC(56â†’256)
              â†“
         FC(256â†’128)
              â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
      â†“               â†“
Value Stream    Advantage Stream
  FC(128â†’32)       FC(128â†’32)
      â†“               â†“
   FC(32â†’1)        FC(32â†’4)
      â†“               â†“
      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    Q(s,a) = V(s) + (A(s,a) - mean(A))
              â†“
         Q-Values (4)
```

### Hamilton Cycle Algorithm

1. **Grid Preparation**: Create nodes at odd coordinates (half-resolution)
2. **Graph Construction**: Build weighted edges between adjacent nodes
3. **MST Generation**: Apply Prim's algorithm to create spanning tree
4. **Cycle Navigation**: Use wall-following to traverse around tree edges
5. **Path Mapping**: Convert to full-resolution Hamiltonian cycle

## ğŸ“š Training Details

### Reward Structure

- **Eating food**: +100 (encourages growth)
- **Following Hamilton (when large)**: +20 (bonus for safety)
- **Space preservation**: Variable (encourages open space)
- **Moving toward food**: +0.1 (small hint when safe)
- **Reducing accessible space**: -0.5 per cell (discourages trapping)
- **Low space ratio**: -1.0 (warns of cramped conditions)
- **Wall collision**: -100 (severe penalty)
- **Self collision**: -100 (severe penalty)

### Hyperparameters

```python
learning_rate = 0.001          # Adam optimizer
gamma = 0.9                    # Discount factor
epsilon_start = 1.0            # Initial exploration
epsilon_end = 0.01             # Minimum exploration
epsilon_decay = 0.995          # Decay rate
memory_size = 100,000          # Replay buffer
batch_size = 1000              # Training batch
hidden_size = 256              # Network width
target_update = 10             # Update frequency
```

### Training Tips

1. **Start with small grids** (10Ã—10) - easier to learn
2. **Use headless mode** for faster training
3. **Monitor episode scores** - should gradually increase
4. **Enable Hamilton guidance** - speeds up early learning
5. **Train for 1000+ episodes** - patience is key
6. **Save checkpoints** - resume training later

## ğŸ“ Project Structure

```
Snake/
â”œâ”€â”€ main.py                    # Interactive menu and launcher
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ README.md                  # This file
â”‚
â”œâ”€â”€ algorithms/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ hamilton_cycle.py     # Hamiltonian cycle generation
â”‚   â””â”€â”€ inspiration.py        # Original cycle visualization
â”‚
â”œâ”€â”€ game/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ environment.py        # RL environment (SnakeEnv)
â”‚   â””â”€â”€ manual_play.py        # Human playable game
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ snake_ai.py           # DQN agent implementation
â”‚   â”œâ”€â”€ train.py              # Training loop
â”‚   â””â”€â”€ watch.py              # Model evaluation
â”‚
â””â”€â”€ demos/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ hamilton_demo.py      # Hamilton cycle demo
```

## âŒ¨ï¸ Controls

### During Gameplay

| Key | Action |
|-----|--------|
| `â†‘` / `W` | Move up |
| `â†“` / `S` | Move down |
| `â†` / `A` | Move left |
| `â†’` / `D` | Move right |
| `R` / `Space` | Restart game |
| `H` | Toggle Hamilton path overlay |
| `F11` | Toggle fullscreen |
| `ESC` | Exit |

### Window Management

- **Drag window edges** - Resize (maintains aspect ratio)
- **F11** - Fullscreen mode
- **ESC** - Exit fullscreen or quit

## âš™ï¸ Configuration

### Grid Size

Customize grid dimensions when prompted:
- Default: 10Ã—10
- Supports rectangular grids (e.g., 20Ã—15)
- Larger grids = harder to learn (more state space)

### Speed

Adjust movement speed (cells per second):
- Manual play: 6 cells/sec (default)
- Hamilton demo: Configurable (1-20)
- Training: Auto-optimized

### Training Episodes

- Quick test: 100 episodes
- Basic training: 1,000 episodes
- Advanced training: 10,000+ episodes

## ğŸ”§ Technical Details

### State Space Complexity

- **Total state space**: ~10^30 possible states
- **Compressed representation**: 124 real-valued features
- **CNN processing**: Extracts spatial patterns from 10Ã—10 grid
- **Feature engineering**: Hand-crafted danger/food signals

### Action Space

4 discrete actions: UP, DOWN, LEFT, RIGHT
- Invalid moves (180Â° turns) automatically prevented
- Safe action masking during inference

### Exploration Strategy

1. **Hamilton-guided**: Follow optimal path 80% of the time (early training)
2. **Epsilon-greedy**: Random exploration with decay
3. **Adaptive decay**: Faster when improving, slower when stuck
4. **Safe action filtering**: Prefer non-collision moves

### Learning Algorithm

**Dueling Double DQN** with:
- **Double DQN**: Reduces overestimation bias
  - Policy network selects action
  - Target network evaluates action
- **Dueling Architecture**: Separates value and advantage
  - V(s): How good is this state?
  - A(s,a): How much better is each action?
  - Q(s,a) = V(s) + (A(s,a) - mean(A))
- **Prioritized Replay**: Samples high-TD-error experiences more
- **Target Network**: Updated every 10 episodes for stability
- **Gradient Clipping**: Prevents exploding gradients

## ğŸ“Š Performance

### Hamilton Cycle

- **Success Rate**: 100% (never dies)
- **Grid Completion**: Fills entire grid every time
- **Speed**: Depends on grid size (10Ã—10 takes ~100 moves)
- **Memory**: O(grid_size) - stores cycle mapping

### Deep Q-Learning

Typical performance after 1000 episodes on 10Ã—10 grid:
- **Average Score**: 15-30 (varies by training run)
- **Max Score**: 40-98 (best agents approach perfection)
- **Training Time**: 1-3 hours (GPU) or 5-10 hours (CPU)
- **Model Size**: ~2 MB

### Hardware Requirements

**Minimum:**
- CPU: Dual-core 2.0 GHz
- RAM: 4 GB
- GPU: Optional (CPU training works)

**Recommended:**
- CPU: Quad-core 3.0 GHz+
- RAM: 8 GB
- GPU: CUDA-compatible (10Ã— faster training)

## ğŸ§ª Advanced Usage

### Custom Grid Sizes

```python
from game.environment import SnakeEnv

env = SnakeEnv(
    render=True,
    grid_width=20,
    grid_height=15,
    speed_cells=8
)
```

### Load Pretrained Model

```python
from training.snake_ai import DQNAgent

agent = DQNAgent(state_size=124, action_size=4)
agent.load('snake_model.pth')
```

### Hamilton Path Only

```python
from algorithms.hamilton_cycle import HamiltonianSnakePlanner

planner = HamiltonianSnakePlanner(grid_width=10, grid_height=10)
next_dir = planner.get_next_direction(head_pos=[5, 5])
```

### Visualize Cycle

```python
from algorithms.hamilton_cycle import visualize_cycle

visualize_cycle(grid_width=10, grid_height=10)
```

## ğŸ› Troubleshooting

### "No module named torch"
```bash
pip install torch --extra-index-url https://download.pytorch.org/whl/cpu
```

### "pygame not found"
```bash
pip install pygame --upgrade
```

### Training is slow
- Use headless mode (disable rendering)
- Reduce grid size
- Use GPU if available
- Decrease batch size

### Model not improving
- Train longer (1000+ episodes minimum)
- Adjust reward structure
- Enable Hamilton guidance
- Check epsilon decay rate

### Window too small/large
- Press F11 for fullscreen
- Drag window edges to resize
- Adjust grid size in configuration

## ğŸ“– Learning Resources

### Reinforcement Learning
- [Sutton & Barto: RL Book](http://incompleteideas.net/book/the-book-2nd.html)
- [Deep Q-Learning Paper (2015)](https://www.nature.com/articles/nature14236)
- [Dueling DQN Paper (2016)](https://arxiv.org/abs/1511.06581)

### Hamiltonian Cycles
- [Hamiltonian Path Problem](https://en.wikipedia.org/wiki/Hamiltonian_path_problem)
- [Prim's Algorithm](https://en.wikipedia.org/wiki/Prim%27s_algorithm)

### Snake AI Strategies
- [Perfect Snake AI Analysis](https://johnflux.com/2015/05/02/nokia-6110-part-3-algorithms/)

## ğŸ¤ Contributing

Contributions welcome! Areas for improvement:
- Multi-snake competitive mode
- Curriculum learning schedules
- Different neural architectures (transformers?)
- Online learning / continuous training
- Tournament mode with leaderboards

## ğŸ“ License

This project is open source and available for educational purposes.

## ğŸ™ Acknowledgments

- Hamilton cycle implementation inspired by classic Nokia Snake algorithms
- Deep Q-Learning based on DeepMind's DQN papers
- Smooth rendering adapted from pygame community examples

## ğŸ“§ Contact

For questions, issues, or suggestions, please open an issue on the project repository.

---

**Made with ğŸ and â¤ï¸**

*Happy Snake Training!* ğŸ®ğŸ¤–